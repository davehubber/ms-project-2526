import os
import sys
import subprocess
import glob
import numpy as np
import pandas as pd
import traci

# This file should be in the same folder as the cleaned TAZ file
# Before running this, ensure that the taz_cleaner.py script has been executed
# to produce 'taz_clean.xml' from the original 'taz.xml'.

# --- CONFIGURATION ---
try:
    if 'SUMO_HOME' in os.environ:
        tools = os.path.join(os.environ['SUMO_HOME'], 'tools')
        sys.path.append(tools)
    else:
        os.environ["SUMO_HOME"] = "/usr/share/sumo"

    SUMO_BINARY = "sumo"
    OD2TRIPS_BINARY = "od2trips"
    DUAROUTER_BINARY = "duarouter"
    
    # Check dependencies
    subprocess.run(["which", SUMO_BINARY], check=True, stdout=subprocess.DEVNULL)
    subprocess.run(["which", DUAROUTER_BINARY], check=True, stdout=subprocess.DEVNULL)
    
except subprocess.CalledProcessError:
    sys.exit("ERROR: SUMO or DUAROUTER not found. Please check your installation.")

NET_FILE = "net.net.xml"
TAZ_FILE = "taz_clean.xml"
DETECTORS_FILE = "detectors.add.xml"
OD_FOLDER = "od-matrixes"
UPDATED_OD_FOLDER = "od-matrixes-updated"
OUTPUT_FILE = "simulation_dataset.csv"      
BACKUP_FILE = "partial_dataset_backup.csv"  

# --- HELPER FUNCTIONS ---

def load_base_matrix(file_path):
    data = []
    try:
        with open(file_path, 'r') as f:
            for line in f:
                stripped = line.strip()
                if not stripped or stripped.startswith('*') or stripped.startswith('$'):
                    continue
                parts = stripped.split()
                if len(parts) >= 3:
                    try:
                        data.append({'from': parts[0], 'to': parts[1], 'count': float(parts[2])})
                    except ValueError:
                        continue
    except FileNotFoundError:
        return pd.DataFrame()
    return pd.DataFrame(data)

def extract_hours_from_filename(filename):
    start_h_int = 0
    try:
        parts = filename.split('.')
        if len(parts) >= 4:
            begin_hour = parts[1]
            end_hour = parts[3]
            start_h_int = int(begin_hour)
            if (end_hour == '23' and begin_hour == '23'): 
                return "23.00", "23.59", 23
            return f"{begin_hour}.00", f"{end_hour}.00", start_h_int
    except Exception:
        pass
    return "0.00", "1.00", 0

def write_perturbed_matrix(df, run_id, original_filename, noise_level=0.15):
    perturbation = np.random.uniform(1.0 - noise_level, 1.0 + noise_level, size=len(df))
    df_perturbed = df.copy()
    df_perturbed['count'] = (df['count'] * perturbation).round(0).astype(int)
    
    begin, end, _ = extract_hours_from_filename(original_filename)
    
    filename_only = f"temp_{original_filename}_{run_id}.txt"
    temp_name = os.path.join(UPDATED_OD_FOLDER, filename_only)
    
    with open(temp_name, 'w') as f:
        f.write("$OR;D2\n")
        f.write("* From-Time  To-Time\n")
        f.write(f"{begin} {end}\n")
        f.write("* Factor\n")
        f.write("1.00\n")
        f.write(f"* Generated by Python script run {run_id}\n")
        
        for _, row in df_perturbed.iterrows():
            f.write(f"{row['from']}\t{row['to']}\t{row['count']}\n")
    
    return temp_name, df_perturbed['count'].values

def generate_trips_file(od_file, run_id):
    
    raw_trips_file = od_file.replace(".txt", ".trips.xml") # Tempor√°rio
    final_rou_file = od_file.replace(".txt", ".rou.xml")   # Validado

    os.makedirs("od2tripslogs", exist_ok=True)
    os.makedirs("duarouterlogs", exist_ok=True)
    
    # Generate trips using od2trips
    cmd_od = [
        OD2TRIPS_BINARY,
        "-n", TAZ_FILE,
        "-d", od_file,
        "-o", raw_trips_file,
        "--no-step-log", "true",
        "--ignore-errors", "true",
        "--error-log", f"od2tripslogs/od2trips_error_log_{run_id}.txt"
    ]
    
    # Generate routes using duarouter
    cmd_dua = [
        DUAROUTER_BINARY,
        "-n", NET_FILE,
        "--route-files", raw_trips_file,
        "--output-file", final_rou_file,
        "--ignore-errors", "true",     
        "--no-step-log", "true",
        "--error-log", f"duarouterlogs/duarouter_error_log_{run_id}.txt"
    ]

    try:
        subprocess.run(cmd_od, check=True, capture_output=True, text=True)
        subprocess.run(cmd_dua, check=False, capture_output=False, text=True)
        if os.path.exists(raw_trips_file):
            os.remove(raw_trips_file)

    except subprocess.CalledProcessError as e:
        print(f"\n[WARNING] Failed to generate routes for: {od_file}")
        # If it fails, try to ensure the final file exists (even if empty)
        if not os.path.exists(final_rou_file):
             with open(final_rou_file, 'w') as f: f.write("<routes/>")
             
    return final_rou_file

def run_simulation(trips_file, start_hour_int):
    STEPS_PER_INTERVAL = 300 
    MAX_INTERVALS = 12        
    
    begin_time_sec = start_hour_int * 3600
    
    cmd = [
        SUMO_BINARY, 
        "-n", NET_FILE, 
        "-a", DETECTORS_FILE, 
        "-r", trips_file,
        "--begin", str(begin_time_sec), 
        "--no-step-log", "true", 
        "--waiting-time-memory", "1000",
        "--time-to-teleport", "-1",
        "--ignore-route-errors", "true",
        "--start"
    ]
    
    try:
        traci.start(cmd)
    except traci.FatalTraCIError:
        return {}
    
    all_detectors = traci.inductionloop.getIDList()
    previous_step_ids = {det: set() for det in all_detectors}
    
    current_interval_data = {
        det: {'counts': 0, 'speeds': [], 'occs': []} 
        for det in all_detectors
    }
    
    final_output = {}
    step_counter = 0 
    interval_index = 0
    
    while traci.simulation.getMinExpectedNumber() > 0:
        try:
            traci.simulationStep()
        except traci.FatalTraCIError:
            break
        
        step_counter += 1

        for det_id in all_detectors:
            current_ids = set(traci.inductionloop.getLastStepVehicleIDs(det_id))
            new_cars = current_ids - previous_step_ids[det_id]
            if new_cars:
                current_interval_data[det_id]['counts'] += len(new_cars)
            previous_step_ids[det_id] = current_ids
            
            s = traci.inductionloop.getLastStepMeanSpeed(det_id)
            if s >= 0: current_interval_data[det_id]['speeds'].append(s)
            
            occ = traci.inductionloop.getLastStepOccupancy(det_id)
            current_interval_data[det_id]['occs'].append(occ)

        if step_counter % STEPS_PER_INTERVAL == 0:
            suffix = f"_i{interval_index}"
            for det_id in all_detectors:
                data = current_interval_data[det_id]
                final_output[f"{det_id}{suffix}_flow"] = data['counts']
                avg_spd = sum(data['speeds'])/len(data['speeds']) if data['speeds'] else 0.0
                final_output[f"{det_id}{suffix}_speed"] = avg_spd
                avg_occ = sum(data['occs'])/len(data['occs']) if data['occs'] else 0.0
                final_output[f"{det_id}{suffix}_occupancy"] = avg_occ
                current_interval_data[det_id] = {'counts': 0, 'speeds': [], 'occs': []}
            
            interval_index += 1
            if interval_index >= MAX_INTERVALS:
                break
    
    if step_counter % STEPS_PER_INTERVAL != 0 and interval_index < MAX_INTERVALS:
        suffix = f"_i{interval_index}"
        for det_id in all_detectors:
            data = current_interval_data[det_id]
            final_output[f"{det_id}{suffix}_flow"] = data['counts']
            avg_spd = sum(data['speeds'])/len(data['speeds']) if data['speeds'] else 0.0
            final_output[f"{det_id}{suffix}_speed"] = avg_spd
            avg_occ = sum(data['occs'])/len(data['occs']) if data['occs'] else 0.0
            final_output[f"{det_id}{suffix}_occupancy"] = avg_occ
        interval_index += 1

    traci.close()
    
    while interval_index < MAX_INTERVALS:
        suffix = f"_i{interval_index}"
        for det_id in all_detectors:
            final_output[f"{det_id}{suffix}_flow"] = 0
            final_output[f"{det_id}{suffix}_speed"] = 0.0
            final_output[f"{det_id}{suffix}_occupancy"] = 0.0
        interval_index += 1
        
    return final_output

# --- MAIN EXECUTION ---
if __name__ == "__main__":
    matrix_files = glob.glob(os.path.join(OD_FOLDER, "*.xml"))
    matrix_files.sort(key=lambda f: int(os.path.basename(f).split('.')[1]))
    
    if not os.path.exists(UPDATED_OD_FOLDER):
        os.makedirs(UPDATED_OD_FOLDER, exist_ok=True)
    
    print(f"--- MODE: 5-MINUTE INTERVALS ---")
    
    dataset_rows = []
    
    if os.path.exists(BACKUP_FILE):
        print(f">> Found backup. Resuming...")
        try:
            df_backup = pd.read_csv(BACKUP_FILE)
            df_backup = df_backup.loc[:, ~df_backup.columns.str.contains('^Unnamed')]
            dataset_rows = df_backup.to_dict('records')
        except Exception:
            dataset_rows = []
            
    count = len(dataset_rows)
    NUM_SAMPLES = 2 
    total = len(matrix_files) * NUM_SAMPLES
    
    for matrix_path in matrix_files:
        filename = os.path.basename(matrix_path)
        completed_runs = sum(1 for row in dataset_rows if row.get('source_matrix') == filename)
        
        if completed_runs >= NUM_SAMPLES:
            print(f"   [SKIP] {filename}")
            continue
        
        print(f"\n>> Processing: {filename}")
        base_df = load_base_matrix(matrix_path)
        _, _, start_h = extract_hours_from_filename(filename)
        
        if base_df.empty: continue

        for i in range(NUM_SAMPLES):
            if i < completed_runs: continue

            count += 1
            print(f"   Run {i+1}/{NUM_SAMPLES} (Total: {count}/{total})...")
            
            try:
                od_txt, od_vector = write_perturbed_matrix(base_df, count, filename)
                
                trips_xml = generate_trips_file(od_txt, count)
                
                outputs = run_simulation(trips_xml, start_h)
                
                row = {'run_id': count, 'source_matrix': filename}
                for idx, val in enumerate(od_vector):
                    row[f"od_{idx}"] = val
                row.update(outputs)
                dataset_rows.append(row)

                pd.DataFrame(dataset_rows).to_csv(BACKUP_FILE, index=False)

            except Exception as e:
                print(f"\n   [FAIL] {e}")
            
            finally:
                pass

    final_df = pd.DataFrame(dataset_rows)
    final_df.to_csv(OUTPUT_FILE, index=False)
    print(f"\n\nDone! Saved to: {OUTPUT_FILE}")